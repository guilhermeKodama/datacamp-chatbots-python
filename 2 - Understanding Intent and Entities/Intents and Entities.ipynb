{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent classification with regex I\n",
    "\n",
    "You'll begin by implementing a very simple technique to recognize intents - looking for the presence of keywords.\n",
    "\n",
    "A dictionary, keywords, has already been defined. It has the intents \"greet\", \"goodbye\", and \"thankyou\" as keys, and lists of keywords as the corresponding values. For example, keywords[\"greet\"] is set to \"[\"hello\",\"hi\",\"hey\"].\n",
    "\n",
    "Also defined is a second dictionary, responses, indicating how the bot should respond to each of these intents. It also has a default response with the key \"default\".\n",
    "\n",
    "The function send_message(), along with the bot and user templates, have also already been defined. Your job in this exercise is to create a dictionary with the intents as keys and regex objects as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': re.compile('hello|hi|hey'), 'goodbye': re.compile('bye|farewell'), 'thankyou': re.compile('thank|thx')}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "keywords = {'greet': ['hello', 'hi', 'hey'], 'goodbye': ['bye', 'farewell'], 'thankyou': ['thank', 'thx']}\n",
    "\n",
    "# Define a dictionary of patterns\n",
    "patterns = {}\n",
    "\n",
    "# Iterate over the keywords dictionary\n",
    "for intent, keys in keywords.items():\n",
    "    # Create regular expressions and compile them into pattern objects\n",
    "    patterns[intent] = re.compile('|'.join(keys))\n",
    "    \n",
    "# Print the patterns\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent classification with regex II\n",
    "\n",
    "With your patterns dictionary created, it's now time to define a function to find the intent of a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello!\n",
      "BOT : Hello you! :)\n",
      "USER : bye byeee\n",
      "BOT : goodbye for now\n",
      "USER : thanks very much!\n",
      "BOT : you are very welcome\n"
     ]
    }
   ],
   "source": [
    "user_template = 'USER : {0}'\n",
    "bot_template = 'BOT : {0}'\n",
    "\n",
    "responses = {'greet': 'Hello you! :)', 'goodbye': 'goodbye for now', 'thankyou': 'you are very welcome', 'default': 'default message'}\n",
    "    \n",
    "def send_message(message):\n",
    "    print(user_template.format(message))\n",
    "    response = respond(message)\n",
    "    print(bot_template.format(response))\n",
    "\n",
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        # Check if the pattern occurs in the message \n",
    "        if re.search(pattern, message):\n",
    "            matched_intent = intent\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent(message)\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]\n",
    "\n",
    "# Send messages\n",
    "send_message(\"hello!\")\n",
    "send_message(\"bye byeee\")\n",
    "send_message(\"thanks very much!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction with regex\n",
    "\n",
    "Now you'll use another simple method, this time for finding a person's name in a sentence, such as \"hello, my name is David Copperfield\".\n",
    "\n",
    "You'll look for the keywords \"name\" or \"call(ed)\", and find capitalized words using regex and assume those are names. Your job in this exercise is to define a find_name() function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is David Copperfield\n",
      "BOT : Hello, David Copperfield!\n",
      "USER : call me Ishmael\n",
      "BOT : Hello, Ishmael!\n",
      "USER : People call me Cassandra\n",
      "BOT : Hello, People Cassandra!\n"
     ]
    }
   ],
   "source": [
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile('(name|call)')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile('[A-Z]{1}[a-z]*')\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall(message)\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name(message)\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)\n",
    "\n",
    "# Send messages\n",
    "send_message(\"my name is David Copperfield\")\n",
    "send_message(\"call me Ishmael\")\n",
    "send_message(\"People call me Cassandra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vectors with spaCy\n",
    "\n",
    "In this exercise you'll get your first experience with word vectors! You're going to use the ATIS dataset, which contains thousands of sentences from real people interacting with a flight booking system.\n",
    "\n",
    "The user utterances are available in the list sentences, and the corresponding intents in labels.\n",
    "\n",
    "Your job is to create a 2D array X with as many rows as there are sentences in the dataset, where each row is a vector describing that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.3.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.0/en_core_web_md-2.3.0.tar.gz#egg=en_core_web_md==2.3.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (2.3.0)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from en_core_web_md==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.0.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (47.3.1.post20200622)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (4.45.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.19.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.0) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install spacy\n",
    "#!python -m spacy download en\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_DIM: 300\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "sentences = [' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', ' what flights are available from pittsburgh to baltimore on thursday morning', ' what is the arrival time in san francisco for the 755 am flight leaving washington', ' cheapest airfare from tacoma to orlando', ' round trip fares from pittsburgh to philadelphia under 1000 dollars', ' i need a flight tomorrow from columbus to minneapolis', ' what kind of aircraft is used on a flight from cleveland to dallas', ' show me the flights from pittsburgh to los angeles on thursday', ' all flights from boston to washington', ' what kind of ground transportation is available in denver', ' show me the flights from dallas to san francisco', ' show me the flights from san diego to newark by way of houston', ' what is the cheapest flight from boston to bwi', ' all flights to baltimore after 6 pm', ' show me the first class fares from boston to denver', ' show me the ground transportation in denver', ' all flights from denver to pittsburgh leaving after 6 pm and before 7 pm', ' i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore', ' please give me the flights from boston to pittsburgh on thursday of next week', ' i would like to fly from denver to pittsburgh on united airlines', ' show me the flights from san diego to newark', ' please list all first class flights on united from denver to baltimore', ' what kinds of planes are used by american airlines', \" i'd like to have some information on a ticket from denver to pittsburgh and atlanta\", \" i'd like to book a flight from atlanta to denver\", ' which airline serves denver pittsburgh and atlanta', \" show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o'clock pm\", ' atlanta ground transportation', ' i also need service from dallas to boston arriving by noon', ' show me the cheapest round trip fare from baltimore to dallas']\n",
    "\n",
    "# Load the spacy model: nlp\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Calculate the length of sentences\n",
    "n_sentences = len(sentences)\n",
    "\n",
    "# Calculate the dimensionality of nlp\n",
    "embedding_dim = nlp.vocab.vectors_length\n",
    "print('VOCAB_DIM:', embedding_dim)\n",
    "\n",
    "# Initialize the array with zeros: X\n",
    "X = np.zeros((n_sentences, embedding_dim))\n",
    "\n",
    "# Iterate over the sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Pass each each sentence to the nlp object to create a document\n",
    "    doc = nlp(sentence)\n",
    "    # Save the document's .vector attribute to the corresponding row in X\n",
    "    X[idx, :] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent classification with sklearn\n",
    "\n",
    "An array X containing vectors describing each of the sentences in the ATIS dataset has been created for you, along with a 1D array y containing the labels. The labels are integers corresponding to the intents in the dataset. For example, label 0 corresponds to the intent atis_flight.\n",
    "\n",
    "Now, you'll use the scikit-learn library to train a classifier on this same dataset. Specifically, you will fit and evaluate a support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.0.5-cp36-cp36m-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from pandas) (1.19.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.0.5\n",
      "Requirement already satisfied: sklearn in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.19.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.5.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4978 entries, 0 to 4977\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype   \n",
      "---  ------   --------------  -----   \n",
      " 0   label    4978 non-null   category\n",
      " 1   message  4978 non-null   string  \n",
      "dtypes: category(1), string(1)\n",
      "memory usage: 44.7 KB\n"
     ]
    }
   ],
   "source": [
    "# TODO create X_train, y_train and X_test from ATIS dataset (https://www.kaggle.com/hassanamin/atis-airlinetravelinformationsystem/kernels)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./atis_intents.csv')\n",
    "df.label = df.label.astype('category')\n",
    "df.message = df.label.astype('string')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (3335, 300)\n",
      "X_test (1643, 300)\n",
      "y_train (3335,)\n",
      "y_test (1643,)\n"
     ]
    }
   ],
   "source": [
    "def build_features(sentences):\n",
    "    # Calculate the length of sentences\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    # Calculate the dimensionality of nlp\n",
    "    embedding_dim = nlp.vocab.vectors_length\n",
    "\n",
    "    # Initialize the array with zeros: X\n",
    "    X = np.zeros((n_sentences, embedding_dim))\n",
    "\n",
    "    # Iterate over the sentences\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        # Pass each each sentence to the nlp object to create a document\n",
    "        doc = nlp(sentence)\n",
    "        # Save the document's .vector attribute to the corresponding row in X\n",
    "        X[idx, :] = doc.vector\n",
    "\n",
    "    return X\n",
    "\n",
    "X = build_features(df['message'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.label, test_size=0.33, random_state=42)\n",
    "\n",
    "print('X_train', X_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                       atis_abbreviation       0.00      0.00      0.00        38\n",
      "                           atis_aircraft       0.00      0.00      0.00        22\n",
      "atis_aircraft#atis_flight#atis_flight_no       0.00      0.00      0.00         1\n",
      "                            atis_airfare       0.00      0.00      0.00       141\n",
      "                            atis_airline       0.00      0.00      0.00        49\n",
      "                            atis_airport       0.00      0.00      0.00         4\n",
      "                           atis_capacity       0.00      0.00      0.00         4\n",
      "                               atis_city       0.00      0.00      0.00         8\n",
      "                           atis_distance       0.00      0.00      0.00         4\n",
      "                             atis_flight       0.76      1.00      0.86      1242\n",
      "                atis_flight#atis_airfare       0.00      0.00      0.00         8\n",
      "                          atis_flight_no       0.00      0.00      0.00         6\n",
      "                        atis_flight_time       0.00      0.00      0.00        16\n",
      "                        atis_ground_fare       0.00      0.00      0.00         3\n",
      "                     atis_ground_service       0.00      0.00      0.00        72\n",
      "                               atis_meal       0.00      0.00      0.00         2\n",
      "                           atis_quantity       0.00      0.00      0.00        21\n",
      "                        atis_restriction       0.00      0.00      0.00         2\n",
      "\n",
      "                                accuracy                           0.76      1643\n",
      "                               macro avg       0.04      0.06      0.05      1643\n",
      "                            weighted avg       0.57      0.76      0.65      1643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a support vector classifier\n",
    "clf = SVC(C=1)\n",
    "\n",
    "# Fit the classifier using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# Count the number of correct predictions\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using spaCy's entity recognizer\n",
    "\n",
    "In this exercise, you'll use spaCy's built-in entity recognizer to extract names, dates, and organizations from search queries. The spaCy library has been imported for you, and its English model has been loaded as nlp.\n",
    "\n",
    "Your job is to define a function called extract_entities(), which takes in a single argument message and returns a dictionary with the included entity types as keys, and the extracted entities as values. The included entity types are contained in a list called include_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DATE': '2010', 'ORG': 'Google', 'PERSON': 'Mary'}\n",
      "{'DATE': '1999', 'ORG': 'MIT', 'PERSON': None}\n"
     ]
    }
   ],
   "source": [
    "# Define included_entities\n",
    "include_entities = ['DATE', 'ORG', 'PERSON']\n",
    "\n",
    "# Define extract_entities()\n",
    "def extract_entities(message):\n",
    "    # Create a dict to hold the entities\n",
    "    ents = dict.fromkeys(include_entities)\n",
    "    # Create a spacy document\n",
    "    doc = nlp(message)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ents:\n",
    "            # Save interesting entities\n",
    "            ents[ent.label_] = ent.text\n",
    "    return ents\n",
    "\n",
    "print(extract_entities('friends called Mary who have worked at Google since 2010'))\n",
    "print(extract_entities('people who graduated from MIT in 1999'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning roles using spaCy's parser\n",
    "\n",
    "In this exercise you'll use spaCy's powerful syntax parser to assign roles to the entities in your users' messages. To do this, you'll define two functions, find_parent_item() and assign_colors(). In doing so, you'll use a parse tree to assign roles, similar to how Alan did in the video.\n",
    "\n",
    "Recall that you can access the ancestors of a word using its .ancestors attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item: jacket has color : red\n",
      "item: jeans has color : blue\n"
     ]
    }
   ],
   "source": [
    "items = ['shoes', 'handback', 'jacket', 'jeans']\n",
    "colors = ['black', 'red', 'blue']\n",
    "# Create the document\n",
    "doc = nlp(\"let's see that jacket in red and some blue jeans\")\n",
    "\n",
    "def entity_type(word):\n",
    "    _type = None\n",
    "    if word.text in colors:\n",
    "        _type = \"color\"\n",
    "    elif word.text in items:\n",
    "        _type = \"item\"\n",
    "    return _type\n",
    "\n",
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent\n",
    "            item =  find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rasa NLU\n",
    "\n",
    "In this exercise, you'll use Rasa NLU to create an interpreter, which parses incoming user messages and returns a set of entities. Your job is to train an interpreter using the MITIE entity recognition model in Rasa NLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasa_nlu in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (0.15.1)\n",
      "Requirement already satisfied: typing~=3.6 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (3.7.4.1)\n",
      "Requirement already satisfied: future~=0.17.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (0.17.1)\n",
      "Requirement already satisfied: ruamel.yaml~=0.15.7 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (0.15.100)\n",
      "Requirement already satisfied: cloudpickle~=0.6.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (0.6.1)\n",
      "Requirement already satisfied: requests~=2.20 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (2.23.0)\n",
      "Requirement already satisfied: coloredlogs~=10.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (10.0)\n",
      "Requirement already satisfied: boto3~=1.9 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (1.14.8)\n",
      "Requirement already satisfied: scikit-learn~=0.20.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (0.20.4)\n",
      "Requirement already satisfied: simplejson~=3.13 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (3.17.0)\n",
      "Requirement already satisfied: jsonschema~=2.6 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (2.6.0)\n",
      "Requirement already satisfied: tqdm~=4.19 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (4.45.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (1.19.0)\n",
      "Requirement already satisfied: matplotlib~=2.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (2.2.5)\n",
      "Requirement already satisfied: gevent~=1.3 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (1.5.0)\n",
      "Requirement already satisfied: packaging~=18.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (18.0)\n",
      "Requirement already satisfied: klein~=17.10 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from rasa_nlu) (17.10.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests~=2.20->rasa_nlu) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests~=2.20->rasa_nlu) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests~=2.20->rasa_nlu) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from requests~=2.20->rasa_nlu) (3.0.4)\n",
      "Requirement already satisfied: humanfriendly>=4.7 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from coloredlogs~=10.0->rasa_nlu) (8.2)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.8 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from boto3~=1.9->rasa_nlu) (1.17.8)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from boto3~=1.9->rasa_nlu) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from boto3~=1.9->rasa_nlu) (0.10.0)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from scikit-learn~=0.20.2->rasa_nlu) (1.5.0)\n",
      "Requirement already satisfied: pytz in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (2018.9)\n",
      "Requirement already satisfied: six>=1.10 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from matplotlib~=2.2->rasa_nlu) (2.8.1)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from gevent~=1.3->rasa_nlu) (0.4.16)\n",
      "Requirement already satisfied: werkzeug in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from klein~=17.10->rasa_nlu) (1.0.1)\n",
      "Requirement already satisfied: Twisted>=15.5 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from klein~=17.10->rasa_nlu) (20.3.0)\n",
      "Requirement already satisfied: incremental in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from klein~=17.10->rasa_nlu) (17.5.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.8->boto3~=1.9->rasa_nlu) (0.15.2)\n",
      "Requirement already satisfied: Automat>=0.3.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (20.2.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (2.0.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (19.3.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (15.1.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (19.0.0)\n",
      "Requirement already satisfied: zope.interface>=4.4.2 in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from Twisted>=15.5->klein~=17.10->rasa_nlu) (5.1.0)\n",
      "Requirement already satisfied: setuptools in /Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages (from zope.interface>=4.4.2->Twisted>=15.5->klein~=17.10->rasa_nlu) (47.3.1.post20200622)\n"
     ]
    }
   ],
   "source": [
    "!pip install rasa_nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/guilherme/.conda/envs/datascience/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`resource_name` must be a string type. Got `<class 'dict'>` instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-36339aba561c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create an interpreter by training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datascience/lib/python3.6/site-packages/rasa_nlu/training_data/loading.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(resource_name, language)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mdata_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mdata_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mds\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_sets\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datascience/lib/python3.6/site-packages/rasa_nlu/utils/__init__.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    101\u001b[0m     If the path points to a file, returns the file.\"\"\"\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/datascience/lib/python3.6/site-packages/rasa_nlu/utils/__init__.py\u001b[0m in \u001b[0;36mlist_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"`resource_name` must be a string type. \"\n\u001b[0;32m---> 82\u001b[0;31m                          \"Got `{}` instead\".format(type(path)))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `resource_name` must be a string type. Got `<class 'dict'>` instead"
     ]
    }
   ],
   "source": [
    "''''\n",
    " UPDATE THIS FUCKING CODE -  OUTDATED RASA CODE\n",
    "''''\n",
    "\n",
    "data =  {'intent': {'name': 'restaurant_search', 'confidence': 0.6406008835918301}, 'entities': [{'start': 18, 'end': 25, 'value': 'mexican', 'entity': 'cuisine', 'extractor': 'ner_crf'}, {'start': 44, 'end': 49, 'value': 'north', 'entity': 'location', 'extractor': 'ner_crf'}], 'intent_ranking': [{'name': 'restaurant_search', 'confidence': 0.6406008835918301}, {'name': 'goodbye', 'confidence': 0.15062421625653372}, {'name': 'affirm', 'confidence': 0.12052679796005}, {'name': 'greet', 'confidence': 0.08824810219158574}], 'text': \"I'm looking for a Mexican restaurant in the North of town\"}\n",
    "\n",
    "# Import necessary modules\n",
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu.config import RasaNLUModelConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "# Create args dictionary\n",
    "args = { \"pipeline\": \"spacy_sklearn\" }\n",
    "\n",
    "# Create a configuration and trainer\n",
    "config = RasaNLUModelConfig(configuration_values=args)\n",
    "trainer = Trainer(config)\n",
    "\n",
    "# Load the training data\n",
    "training_data = load_data(data)\n",
    "\n",
    "# Create an interpreter by training the model\n",
    "interpreter = trainer.train(training_data)\n",
    "\n",
    "# Test the interpreter\n",
    "print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-efficient entity recognition\n",
    "\n",
    "Most systems for extracting entities from text are built to extract 'Universal' things like names, dates, and places. But you probably don't have enough training data for your bot to make these systems perform well!\n",
    "\n",
    "In this exercise, you'll activate the MITIE entity recognizer inside Rasa to extract restaurants-related entities using a very small amount of training data. A dictionary args has already been defined for you, along with a training_data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    " UPDATE THIS FUCKING CODE -  OUTDATED RASA CODE\n",
    "''''\n",
    "\n",
    "# Import necessary modules\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "pipeline = [\n",
    "    \"nlp_spacy\",\n",
    "    \"tokenizer_spacy\",\n",
    "    \"ner_crf\"\n",
    "]\n",
    "\n",
    "# Create a config that uses this pipeline\n",
    "config = RasaNLUConfig(cmdline_args={\"pipeline\": pipeline})\n",
    "\n",
    "# Create a trainer that uses this config\n",
    "trainer = Trainer(config)\n",
    "\n",
    "# Create an interpreter by training the model\n",
    "interpreter = trainer.train(training_data)\n",
    "\n",
    "# Parse some messages\n",
    "print(interpreter.parse(\"show me Chinese food in the centre of town\"))\n",
    "print(interpreter.parse(\"I want an Indian restaurant in the west\"))\n",
    "print(interpreter.parse(\"are there any good pizza places in the center?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
